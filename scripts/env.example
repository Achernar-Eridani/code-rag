# ===== LLM provider switches =====
# local | openai
export RAG_LLM_PROVIDER=local

# ===== Local llama.cpp server (OpenAI-compatible) =====
# llama.cpp server base (OpenAI /v1 compatible)
export LOCAL_LLM_BASE="http://127.0.0.1:8081/v1"
# The model name string you want to expose to the API
export LOCAL_LLM_MODEL="qwen2.5-coder-7b-instruct-q4_k_m"

# (Optional) OpenAI fallback â€” keep empty for pure local
export OPENAI_API_KEY=""

# ===== API server =====
export API_HOST="127.0.0.1"
export API_PORT="8000"
