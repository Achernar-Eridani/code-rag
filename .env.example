# ==============================================
# CodeRAG Agent Configuration
# ==============================================

# 1. Choose your LLM Provider
# Options: openai | qwen_api | local
# 'openai' is recommended for the best Agent experience.
RAG_LLM_PROVIDER=openai

# ==============================================
# [Option 1] OpenAI (Default)
# ==============================================
OPENAI_API_KEY=
RAG_LLM_MODEL=gpt-4o-mini

# ==============================================
# [Option 2] Qwen API (DashScope)
# ==============================================
# DASHSCOPE_API_KEY=sk-your-dashscope-key
# QWEN_API_MODEL=qwen2.5-coder-7b-instruct

# ==============================================
# [Option 3] Local LLM (llama.cpp + GGUF)
# ⚠️ Only required if RAG_LLM_PROVIDER=local
# ⚠️ Ensure the model file exists in ./models/
# ==============================================
# QWEN_GGUF_PATH=./models/qwen2.5-coder-7b-instruct-q4_k_m.gguf
# LLAMA_CTX=8192
# LLAMA_N_GPU_LAYERS=0   # Set to >0 if you have a GPU (e.g., 33 for 7B models)
# LLAMA_THREADS=8        # Adjust based on your CPU cores

# ==============================================
# Infrastructure (Docker internal URLs)
# ==============================================
REDIS_URL=redis://redis:6379/0
# If running local llama inside docker:
LOCAL_LLM_BASE=http://llama:8080/v1